{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_series head \n",
      "      series_id  step           timestamp  anglez    enmo timezone\n",
      "0  038441c925bb     0 2018-08-14 15:30:00  2.6367  0.0217    -0400\n",
      "1  038441c925bb     1 2018-08-14 15:30:05  2.6368  0.0215    -0400\n",
      "2  038441c925bb     2 2018-08-14 15:30:10  2.6370  0.0216    -0400\n",
      "3  038441c925bb     3 2018-08-14 15:30:15  2.6368  0.0213    -0400\n",
      "4  038441c925bb     4 2018-08-14 15:30:20  2.6368  0.0215    -0400\n",
      "train_events head \n",
      "      series_id  night   event     step                  timestamp\n",
      "0  038441c925bb      1   onset   4992.0  2018-08-14 22:26:00-04:00\n",
      "1  038441c925bb      1  wakeup  10932.0  2018-08-15 06:41:00-04:00\n",
      "2  038441c925bb      2   onset  20244.0  2018-08-15 19:37:00-04:00\n",
      "3  038441c925bb      2  wakeup  27492.0  2018-08-16 05:41:00-04:00\n",
      "4  038441c925bb      3   onset  39996.0  2018-08-16 23:03:00-04:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekel\\AppData\\Local\\Temp\\ipykernel_27764\\1344991872.py:8: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise a warning unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  train_events['timestamp'] = pd.to_datetime(train_events['timestamp'], format='%Y-%m-%dT%H:%M:%S%z')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_series = pd.read_parquet('../data/train_series_datecorrected.parquet')\n",
    "print(f\"train_series head \\n{train_series.head()}\")\n",
    "# Load the training events data\n",
    "train_events = pd.read_csv('../data/train_events.csv')\n",
    "train_events['timestamp'] = pd.to_datetime(train_events['timestamp'], format='%Y-%m-%dT%H:%M:%S%z')\n",
    "print(f\"train_events head \\n{train_events.head()}\")\n",
    "\n",
    "# train_series.sort_values(by=['series_id', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with optimized types\n",
    "train_series['anglez'] = train_series['anglez'].astype('float32')\n",
    "train_series['enmo'] = train_series['enmo'].astype('float32')\n",
    "\n",
    "# Define window sizes for rolling calculations\n",
    "window_sizes = [5, 30, 60]\n",
    "\n",
    "# Group by 'series_id' to ensure continuity\n",
    "grouped = train_series.groupby('series_id')\n",
    "\n",
    "# Initialize a counter for naming the saved files\n",
    "chunk_counter = 0\n",
    "\n",
    "for name, group in grouped:\n",
    "    for window in window_sizes:\n",
    "        # Rolling mean\n",
    "        group[f'anglez_rolling_mean_{window}'] = group['anglez'].rolling(window).mean()\n",
    "        group[f'enmo_rolling_mean_{window}'] = group['enmo'].rolling(window).mean()\n",
    "\n",
    "        # Rolling standard deviation\n",
    "        group[f'anglez_rolling_std_{window}'] = group['anglez'].rolling(window).std()\n",
    "        group[f'enmo_rolling_std_{window}'] = group['enmo'].rolling(window).std()\n",
    "\n",
    "    # Save the processed group\n",
    "    group.to_parquet(f'../data/temp/engineered_chunk_{chunk_counter}.parquet')\n",
    "    chunk_counter += 1\n",
    "\n",
    "    # Clear memory\n",
    "    del group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load each chunk with Dask\n",
    "dask_chunks = [dd.read_parquet(f'../data/temp/engineered_chunk_{i}.parquet') for i in range(chunk_counter)]\n",
    "\n",
    "# Concatenate using Dask\n",
    "engineered_series = dd.concat(dask_chunks, axis=0, ignore_index=True)\n",
    "\n",
    "# save it\n",
    "engineered_series.to_parquet('../data/train_series_engineered_dask.parquet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
