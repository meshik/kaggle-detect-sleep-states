{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_series head \n",
      "      series_id  step           timestamp  anglez    enmo timezone\n",
      "0  038441c925bb     0 2018-08-14 15:30:00  2.6367  0.0217    -0400\n",
      "1  038441c925bb     1 2018-08-14 15:30:05  2.6368  0.0215    -0400\n",
      "2  038441c925bb     2 2018-08-14 15:30:10  2.6370  0.0216    -0400\n",
      "3  038441c925bb     3 2018-08-14 15:30:15  2.6368  0.0213    -0400\n",
      "4  038441c925bb     4 2018-08-14 15:30:20  2.6368  0.0215    -0400\n",
      "train_events head \n",
      "      series_id  night   event     step                  timestamp\n",
      "0  038441c925bb      1   onset   4992.0  2018-08-14 22:26:00-04:00\n",
      "1  038441c925bb      1  wakeup  10932.0  2018-08-15 06:41:00-04:00\n",
      "2  038441c925bb      2   onset  20244.0  2018-08-15 19:37:00-04:00\n",
      "3  038441c925bb      2  wakeup  27492.0  2018-08-16 05:41:00-04:00\n",
      "4  038441c925bb      3   onset  39996.0  2018-08-16 23:03:00-04:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekel\\AppData\\Local\\Temp\\ipykernel_14856\\1344991872.py:8: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise a warning unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  train_events['timestamp'] = pd.to_datetime(train_events['timestamp'], format='%Y-%m-%dT%H:%M:%S%z')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_series = pd.read_parquet('../data/train_series_datecorrected.parquet')\n",
    "print(f\"train_series head \\n{train_series.head()}\")\n",
    "# Load the training events data\n",
    "train_events = pd.read_csv('../data/train_events.csv')\n",
    "train_events['timestamp'] = pd.to_datetime(train_events['timestamp'], format='%Y-%m-%dT%H:%M:%S%z')\n",
    "print(f\"train_events head \\n{train_events.head()}\")\n",
    "\n",
    "# train_series.sort_values(by=['series_id', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data with optimized types\n",
    "train_series['anglez'] = train_series['anglez'].astype('float32')\n",
    "train_series['enmo'] = train_series['enmo'].astype('float32')\n",
    "\n",
    "# Define window sizes for rolling calculations\n",
    "window_sizes = [5, 10, 30, 60, 120]  # Adjust these sizes as needed\n",
    "\n",
    "# Process in chunks to manage memory usage\n",
    "chunk_size = 1000000  # adjust this number as needed\n",
    "num_chunks = (train_series.shape[0] // chunk_size) + 1\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Extract a chunk of data and make an explicit copy\n",
    "    chunk = train_series[i*chunk_size:(i+1)*chunk_size].copy()\n",
    "\n",
    "    # Compute rolling features for each window size\n",
    "    for window in window_sizes:\n",
    "        # Rolling mean\n",
    "        chunk[f'anglez_rolling_mean_{window}'] = chunk.groupby('series_id')['anglez'].transform(lambda x: x.rolling(window).mean())\n",
    "        chunk[f'enmo_rolling_mean_{window}'] = chunk.groupby('series_id')['enmo'].transform(lambda x: x.rolling(window).mean())\n",
    "\n",
    "        # Rolling standard deviation\n",
    "        chunk[f'anglez_rolling_std_{window}'] = chunk.groupby('series_id')['anglez'].transform(lambda x: x.rolling(window).std())\n",
    "        chunk[f'enmo_rolling_std_{window}'] = chunk.groupby('series_id')['enmo'].transform(lambda x: x.rolling(window).std())\n",
    "\n",
    "        # consider adding more features\n",
    "\n",
    "    # save each processed chunk to be concatenated later\n",
    "    chunk.to_parquet(f'../data/temp/engineered_chunk_{i}.parquet')\n",
    "\n",
    "    # Clear memory\n",
    "    del chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load each chunk with Dask\n",
    "dask_chunks = [dd.read_parquet(f'../data/temp/engineered_chunk_{i}.parquet') for i in range(num_chunks)]\n",
    "\n",
    "# Concatenate using Dask\n",
    "engineered_series = dd.concat(dask_chunks, axis=0, ignore_index=True)\n",
    "\n",
    "# save it\n",
    "engineered_series.to_parquet('../data/train_series_engineered_dask.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column assignment doesn't support type numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dekel\\kaggle-detect-sleep-states\\notebooks\\feature_engineering.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dekel/kaggle-detect-sleep-states/notebooks/feature_engineering.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dekel/kaggle-detect-sleep-states/notebooks/feature_engineering.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m le \u001b[39m=\u001b[39m LabelEncoder()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dekel/kaggle-detect-sleep-states/notebooks/feature_engineering.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_data[\u001b[39m'\u001b[39;49m\u001b[39mevent_encoded\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39mfit_transform(train_data[\u001b[39m'\u001b[39m\u001b[39mevent\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\dekel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\core.py:5100\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   5098\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mItem assignment with \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(key)\u001b[39m}\u001b[39;00m\u001b[39m not supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5099\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 5100\u001b[0m     df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massign(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: value})\n\u001b[0;32m   5102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdask \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdask\n\u001b[0;32m   5103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39m_name\n",
      "File \u001b[1;32mc:\\Users\\dekel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\core.py:5527\u001b[0m, in \u001b[0;36mDataFrame.assign\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   5518\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   5519\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m   5520\u001b[0m         \u001b[39misinstance\u001b[39m(v, Scalar)\n\u001b[0;32m   5521\u001b[0m         \u001b[39mor\u001b[39;00m is_series_like(v)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5525\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(v, Array)\n\u001b[0;32m   5526\u001b[0m     ):\n\u001b[1;32m-> 5527\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   5528\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn assignment doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt support type \u001b[39m\u001b[39m{\u001b[39;00mtypename(\u001b[39mtype\u001b[39m(v))\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   5529\u001b[0m         )\n\u001b[0;32m   5530\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(v):\n\u001b[0;32m   5531\u001b[0m         kwargs[k] \u001b[39m=\u001b[39m v(data)\n",
      "\u001b[1;31mTypeError\u001b[0m: Column assignment doesn't support type numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# MODEL TRAINING\n",
    "# Merge events data for labels\n",
    "train_series = dd.read_parquet('../data/train_series_engineered_dask.parquet')\n",
    "print(f\"train_series head \\n{train_series.head()}\")\n",
    "train_data = train_series.merge(train_events, on=['series_id', 'step'], how='left')\n",
    "print(f\"train_data head \\n{train_data.head()}\")\n",
    "train_data['event'] = train_data['event'].fillna('no_event')  # Fill NaN with 'no_event'\n",
    "\n",
    "# Encode target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_data['event_encoded'] = le.fit_transform(train_data['event'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data.drop(['event', 'event_encoded', 'timestamp', 'series_id'], axis=1)\n",
    "y = train_data['event_encoded']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred, target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
