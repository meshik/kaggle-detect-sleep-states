{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dekel\\AppData\\Local\\Temp\\ipykernel_20880\\3454372736.py:7: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise a warning unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  train_events['timestamp'] = pd.to_datetime(train_events['timestamp'], format='%Y-%m-%dT%H:%M:%S%z')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_series = pd.read_parquet('../data/train_series_datecorrected.parquet')\n",
    "print(f\"train_series head \\n{train_series.head()}\")\n",
    "# Load the training events data\n",
    "train_events = pd.read_csv('../data/train_events.csv')\n",
    "train_events['timestamp'] = pd.to_datetime(train_events['timestamp'], format='%Y-%m-%dT%H:%M:%S%z')\n",
    "print(f\"train_events head \\n{train_events.head()}\")\n",
    "\n",
    "# train_series.sort_values(by=['series_id', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data with optimized types\n",
    "train_series['anglez'] = train_series['anglez'].astype('float32')\n",
    "train_series['enmo'] = train_series['enmo'].astype('float32')\n",
    "\n",
    "# Define window sizes for rolling calculations\n",
    "window_sizes = [5, 10, 30, 60, 120]  # Adjust these sizes as needed\n",
    "\n",
    "# Process in chunks to manage memory usage\n",
    "chunk_size = 1000000  # adjust this number as needed\n",
    "num_chunks = (train_series.shape[0] // chunk_size) + 1\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Extract a chunk of data and make an explicit copy\n",
    "    chunk = train_series[i*chunk_size:(i+1)*chunk_size].copy()\n",
    "\n",
    "    # Compute rolling features for each window size\n",
    "    for window in window_sizes:\n",
    "        # Rolling mean\n",
    "        chunk[f'anglez_rolling_mean_{window}'] = chunk.groupby('series_id')['anglez'].transform(lambda x: x.rolling(window).mean())\n",
    "        chunk[f'enmo_rolling_mean_{window}'] = chunk.groupby('series_id')['enmo'].transform(lambda x: x.rolling(window).mean())\n",
    "\n",
    "        # Rolling standard deviation\n",
    "        chunk[f'anglez_rolling_std_{window}'] = chunk.groupby('series_id')['anglez'].transform(lambda x: x.rolling(window).std())\n",
    "        chunk[f'enmo_rolling_std_{window}'] = chunk.groupby('series_id')['enmo'].transform(lambda x: x.rolling(window).std())\n",
    "\n",
    "        # consider adding more features\n",
    "\n",
    "    # save each processed chunk to be concatenated later\n",
    "    chunk.to_parquet(f'../data/temp/engineered_chunk_{i}.parquet')\n",
    "\n",
    "    # Clear memory\n",
    "    del chunk\n",
    "\n",
    "# Load and concatenate the processed chunks\n",
    "engineered_series = pd.concat([pd.read_parquet(f'../data/temp/engineered_chunk_{i}.parquet') for i in range(num_chunks)], ignore_index=True)\n",
    "\n",
    "\n",
    "# Save the engineered features for use in the next steps\n",
    "engineered_series.to_parquet('../data/train_series_engineered.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TRAINING\n",
    "# Merge events data for labels\n",
    "train_data = train_series.merge(train_events, on=['series_id', 'step'], how='left')\n",
    "train_data['event'] = train_data['event'].fillna('no_event')  # Fill NaN with 'no_event'\n",
    "\n",
    "# Encode target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_data['event_encoded'] = le.fit_transform(train_data['event'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data.drop(['event', 'event_encoded', 'timestamp', 'series_id'], axis=1)\n",
    "y = train_data['event_encoded']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred, target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
